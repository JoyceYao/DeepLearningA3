\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

\usepackage{nips_2016}
%\usepackage[final]{nips_2016} % produce camera-ready copy

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2016}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{mathtools}
\usepackage{dsfont}

\title{Deep Learning Assignment 3}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Chung-Ling Yao \\
  \texttt{cly264@nyu.edu} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\section{General Questions}

\paragraph{(a)}

\paragraph{(b)}


\section{Softmax regression gradient calculation}
\paragraph{(a)}

By chain rule, we have:
\begin{equation}
\frac{\partial l}{\partial W_{i,j}} = \frac{\partial l}{\partial \hat{y_i}} \frac{\partial \hat{y_i}}{\partial W_{i,j}}
\end{equation}

\begin{equation}
\frac{\partial l}{\partial \hat{y_i}} 
= \frac{\partial }{\partial \hat{y_i}} (-\sum\nolimits_j {y_j \log \hat{y_j}})
\end{equation}

When $i \neq j$, the elements in the loss function are constant compared to $y_i$.
We can just consider the element that $i=j$.

\begin{equation}
\frac{\partial l}{\partial \hat{y_i}} 
= \frac{\partial }{\partial \hat{y_i}} (- {y_i \log \hat{y_i}})
= \frac{-y_i}{(\ln 10) \hat{y_i}}
= \frac{-y_i}{\hat{y_i}}
\end{equation}

From assignment 1, we have:
\begin{equation}
(X_{out})_i = \frac{exp(\beta (X_{in})_i)}{\sum\nolimits_j exp(\beta (X_{in})_j)}
\end{equation}

and also
\begin{equation}
\frac{\partial (X_{out})_i }{\partial (X_{in})_i }
= \frac{\partial}{\partial (X_{in})_i} \frac{exp(\beta (X_{in})_i)}{\sum\nolimits_j exp(\beta (X_{in})_j)} = \beta (X_{out})_i (1-(X_{out})_i)
\end{equation}

So 
\begin{equation}
\frac{\partial \hat{y_i}}{\partial W_{i,j}} 
= \frac{\partial }{\partial W_{i,j}} \frac{exp(W_{i}x+b_i)}{\sum\nolimits_k exp(W_{k}x+b_k)} \\
= x_{j} \hat{y_i} (1- \hat{y_i})
\end{equation}

Now we can calculate $\frac{\partial l}{\partial W_{i,j}}$
\begin{equation}
\frac{\partial l}{\partial W_{i,j}} = \frac{\partial l}{\partial \hat{y_i}} \frac{\partial \hat{y_i}}{\partial W_{i,j}}
= \frac{-y_i}{\hat{y_i}} x_{j} \hat{y_i} (1- \hat{y_i})
= {-y_i} {x_{j}} (1- \hat{y_i})
\end{equation}


\paragraph{(b)}
When $y_{c1} = 1$ and $\hat{y_{c2}} = 1$ and $c1 \neq c2$, the model has wrong output with confidence 100\% \\
for example: \\
model output = [1, 0] \\ 
true label = [0, 1]  \\

From the loss function in part(a), we will get a very very large loss at $y_{c1} \log \hat{y_{c1}}$ and get 0 error in other positions.
As for the gradient, since
\begin{equation}
\frac{\partial l}{\partial W_{i,j}} = {-y_i} {x_{j}} (1- \hat{y_i})
\end{equation}
when $i=c1$, it will increase the W value that contribute to the correct label, otherwise the W value remain unchanged.
With the softmax function, the $\hat y_i$ for the wrong label will be reduced and the correct the $\hat y_i$ will increase in next forward propagation.


\section{Chain rule}
\paragraph{(a)}
Let
\begin{equation}
f = \frac{a}{b} 
\end{equation}

\begin{equation}
a = x^2 + \sigma{(y)}
\end{equation}

\begin{equation}
b = 3x + y - \sigma{(x)}
\end{equation}

\begin{equation}
\frac{\partial f}{\partial x} 
= \frac{\frac{\partial a}{\partial x} \cdot b - \frac{\partial b}{\partial x} \cdot a }{b^2}  
= \frac{2x \cdot b - (3 - \frac{\partial \sigma{(x)}}{\partial x}) \cdot a}{b^2}
\end{equation}

\begin{equation}
\frac{\partial f}{\partial y} 
= \frac{\frac{\partial a}{\partial y} \cdot b - \frac{\partial b}{\partial y} \cdot a }{b^2} 
= \frac{\frac{\partial \sigma{(y)}}{\partial y} \cdot b - a}{b^2}
\end{equation}

Where 
\begin{equation}
\frac{\partial \sigma{(x)}}{\partial x} = \sigma{(x)} \cdot {(1-\sigma{(x)})}
\end{equation}

\paragraph{(b)}
For $x = 1, y = 0$,
\begin{equation}
\sigma{(x)} = \sigma{(1)} = 0.731
\end{equation}

\begin{equation}
\sigma{(y)} = \sigma{(0)} = 0.5
\end{equation}

\begin{equation}
a = 1 + \sigma{(0)} = 1.5
\end{equation}

\begin{equation}
b = 3*1 + 0 - \sigma{(1)} = 2.269
\end{equation}

So we can calculate derivitive of f by:
\begin{equation}
\frac{\partial f}{\partial x} = \frac{2*1*2.269-(3-0.731*(1-0.731))*1.5}{2.269^2} = 0.0646
\end{equation}

\begin{equation}
\frac{\partial f}{\partial y} = \frac{0.5*(1-0.5)*2.269-1.5}{2.269^2} = -0.181
\end{equation}

\section{Variants of pooling}

\paragraph{(a)}
SpatialMaxPooling
SpatialAveragePooling
SpatialAdaptivePooling
\paragraph{(b)}
\paragraph{(c)}

\section{Convolution}

\paragraph{(a)}
Assume we use zero padding and step size = 1, then:
$(5-3+1)*(5-3+1) = 9$ values will be generated.

\paragraph{(b)}
Let X be a 3x3 matrix on Image Matrix and W is the 3x3 convolution filter. \\
According to the definition of convolution, each element in the output is the point product of these two matrix.
\begin{equation}
F = \sum W \cdot \ast X
\end{equation}

For example:
$F_11 = 4*4+3*5+3*2+5*3+5*3+5*2+2*4+4*3+3*4= 109$

So we have the output F =
$\begin{pmatrix}
  109 & 92 & 72 \\
  108 & 85 & 74 \\
  110 & 74 & 79
\end{pmatrix}$

\paragraph{(c)}

Let $\frac{\partial E}{\partial X^(i-1)} = $
$\begin{pmatrix}
  1 & 1 & 1 \\
  1 & 1 & 1 \\
  1 & 1 & 1
\end{pmatrix}$

The definition of F = 
$\begin{pmatrix}
  \sum\limits_{i=1}^{3}\sum\limits_{j=1}^{3} {X_ij W_ij} & \sum\limits_{i=2}^{4}\sum\limits_{j=1}^{3} {X_ij W_ij} & \sum\limits_{i=3}^{5}\sum\limits_{j=1}^{3} {X_ij W_ij} \\
  \sum\limits_{i=1}^{3}\sum\limits_{j=2}^{4} {X_ij W_ij} & \sum\limits_{i=2}^{4}\sum\limits_{j=2}^{4} {X_ij W_ij} & \sum\limits_{i=3}^{5}\sum\limits_{j=2}^{4} {X_ij W_ij} \\
  \sum\limits_{i=1}^{3}\sum\limits_{j=3}^{5} {X_ij W_ij} & \sum\limits_{i=2}^{4}\sum\limits_{j=3}^{5} {X_ij W_ij} & \sum\limits_{i=3}^{5}\sum\limits_{j=3}^{5} {X_ij W_ij} \\
\end{pmatrix}$

\begin{equation}
\frac{\partial E}{\partial X_ij} = \frac{\partial}{\partial X_ij}{...} = \sum\nolimits {W_ij}{ \frac{\partial E}{\partial X^(i-1)}}
\end{equation}

result = 
$\begin{pmatrix}
  4   & 7  & 10 & 6  & 3  \\
  9   & 17 & 25 & 16 & 8  \\
  11  & 23 & 34 & 23 & 11 \\
  7   & 16 & 24 & 17 & 8  \\
  2   & 6  & 9  & 7  & 3  \\
\end{pmatrix}$

\section{Optimization}
\paragraph{(a)}
\paragraph{(b)}
\paragraph{(c)}
\paragraph{(d)}

\section{Top-k error}
Top-k error: the fraction of test images for which the correct label is not among the k labels considered most probable by the model.

\begin{equation}
E = {\frac{1}{m} \sum\limits_{i=1}^{m} \mathds{1}_{R_i > k} } 
\end{equation}

Where function R counts the number of probability for prediction $\hat y_c$ that's larger than the probability of true label $\hat y_L$ for each test image.
\begin{equation}
R = \sum\nolimits_{c \in C} \mathds{1}_{ \hat y_c > \hat y_L }
\end{equation}

The top-1 errors represents the error rate that the prediction is not the same with true label. 
This is useful to compare the performance of different models. However, it cannot show how how good the model is in general.
Sometimes the categories in a image may be ambiguous and it may be described as multiple labels. The order of several possible labels is ambiguous.
Using top-5 errors provides a more general understanding of learning ability of the model. 

\section{t-SNE}
\paragraph{(a)}
The crowding problem happens when we try to project high dimentional dataset into lower dimentional space. 
The distance between records in lower dimentional space will be much smaller than in high dimentional space.
In some clustering algorithms, the records will form a large group in the center and fail to produce nature clustering.
t-SNE use heavy-tailed distribution to reduce this problem. 
It convert distances into probabilities using a Guassian distribution in high-dimentional space.
Then use a much heavier tails probability distribution in the low-dimentional map.
This allows a moderate distance in the high-dimentional space to be modeled by a much larger distance in low dimention.

\paragraph{(b)}
\begin{equation}
\frac{\partial C}{\partial y_i} = \frac{\partial}{\partial X_ij}{...} = \sum\nolimits {W_ij}{ \frac{\partial E}{\partial X^(i-1)}}
\end{equation}

\section{Proximal gradient decent}

\paragraph{(a)}
\paragraph{(b)}
\paragraph{(c)}
\paragraph{(d)}


\end{document}