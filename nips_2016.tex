\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

\usepackage{nips_2016}
%\usepackage[final]{nips_2016} % produce camera-ready copy

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2016}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{mathtools}
\usepackage{dsfont}

\title{Deep Learning Assignment 3}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Chung-Ling Yao \\
  \texttt{cly264@nyu.edu} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\section{General Questions}

\paragraph{(a)}

\paragraph{(b)}


\section{Softmax regression gradient calculation}
\paragraph{(a)}
\paragraph{(b)}

\section{Chain rule}
\paragraph{(a)}
Let
\begin{equation}
f = \frac{a}{b} 
\end{equation}

\begin{equation}
a = x^2 + \sigma{(y)}
\end{equation}

\begin{equation}
b = 3x + y - \sigma{(x)}
\end{equation}

\begin{equation}
\frac{\partial f}{\partial x} 
= \frac{\frac{\partial a}{\partial x} \cdot b - \frac{\partial b}{\partial x} \cdot a }{b^2}  
= \frac{2x \cdot b - (3 - \frac{\partial \sigma{(x)}}{\partial x}) \cdot a}{b^2}
\end{equation}

\begin{equation}
\frac{\partial f}{\partial y} 
= \frac{\frac{\partial a}{\partial y} \cdot b - \frac{\partial b}{\partial y} \cdot a }{b^2} 
= \frac{\frac{\partial \sigma{(y)}}{\partial y} \cdot b - a}{b^2}
\end{equation}

Where 
\begin{equation}
\frac{\partial \sigma{(x)}}{\partial x} = \sigma{(x)} \cdot {(1-\sigma{(x)})}
\end{equation}

\paragraph{(b)}
For $x = 1, y = 0$,
\begin{equation}
\sigma{(x)} = \sigma{(1)} = 0.269
\end{equation}

\begin{equation}
\sigma{(y)} = \sigma{(0)} = 0.5
\end{equation}

\begin{equation}
a = 1 + \sigma{(0)} = 1.5
\end{equation}

\begin{equation}
b = 3*1 + 0 - \sigma{(1)} = 2.731
\end{equation}

So we can calculate derivitive of f by:
\begin{equation}
\frac{\partial f}{\partial x} = \frac{2*1*2.731-(3-0.269*(1-0.269))*1.5}{2.731^2} = 1.633
\end{equation}

\begin{equation}
\frac{\partial f}{\partial y} = \frac{0.5*(1-0.5)*2.731-1.5}{2.731^2} = -0.1096
\end{equation}

\section{Variants of pooling}

\paragraph{(a)}
SpatialMaxPooling
SpatialAveragePooling
SpatialAdaptivePooling
\paragraph{(b)}
\paragraph{(c)}

\section{Convolution}

\paragraph{(a)}
Assume we use zero padding and step size = 1, then:
$(5-3+1)*(5-3+1) = 9$ values will be generated.

\paragraph{(b)}
Let X be a 3x3 matrix on Image Matrix and W is the 3x3 convolution filter. \\
According to the definition of convolution, each element in the output is the point product of these two matrix.
\begin{equation}
F = \sum W \cdot \ast X
\end{equation}

For example:
$F_11 = 4*4+3*5+3*2+5*3+5*3+5*2+2*4+4*3+3*4= 109$

So we have the output F =
$\begin{pmatrix}
  109 & 92 & 72 \\
  108 & 85 & 74 \\
  110 & 74 & 79
\end{pmatrix}$

\paragraph{(c)}

Let $\frac{\partial E}{\partial X^(i-1)} = $
$\begin{pmatrix}
  1 & 1 & 1 \\
  1 & 1 & 1 \\
  1 & 1 & 1
\end{pmatrix}$

The definition of F = 
$\begin{pmatrix}
  \sum\limits_{i=1}^{3}\sum\limits_{j=1}^{3} {X_ij W_ij} & \sum\limits_{i=2}^{4}\sum\limits_{j=1}^{3} {X_ij W_ij} & \sum\limits_{i=3}^{5}\sum\limits_{j=1}^{3} {X_ij W_ij} \\
  \sum\limits_{i=1}^{3}\sum\limits_{j=2}^{4} {X_ij W_ij} & \sum\limits_{i=2}^{4}\sum\limits_{j=2}^{4} {X_ij W_ij} & \sum\limits_{i=3}^{5}\sum\limits_{j=2}^{4} {X_ij W_ij} \\
  \sum\limits_{i=1}^{3}\sum\limits_{j=3}^{5} {X_ij W_ij} & \sum\limits_{i=2}^{4}\sum\limits_{j=3}^{5} {X_ij W_ij} & \sum\limits_{i=3}^{5}\sum\limits_{j=3}^{5} {X_ij W_ij} \\
\end{pmatrix}$

\begin{equation}
\frac{\partial E}{\partial X_ij} = \frac{\partial}{\partial X_ij}{...} = \sum\nolimits {W_ij}{ \frac{\partial E}{\partial X^(i-1)}}
\end{equation}

result = 
$\begin{pmatrix}
  4   & 7  & 10 & 6  & 3  \\
  9   & 17 & 25 & 16 & 8  \\
  11  & 23 & 34 & 23 & 11 \\
  7   & 16 & 24 & 17 & 8  \\
  2   & 6  & 9  & 7  & 3  \\
\end{pmatrix}$

\section{Optimization}
\paragraph{(a)}
\paragraph{(b)}
\paragraph{(c)}
\paragraph{(d)}

\section{Top-k error}
Top-k error: the fraction of test images for which the correct label is not among the k labels considered most probable by the model.

\begin{equation}
E = {\frac{1}{m} \sum\limits_{i=1}^{m} \mathds{1}_{R_i > k} } 
\end{equation}

Where function R counts the number of probability for prediction $\hat y_c$ that's larger than the probability of true label $\hat y_L$ for each test image.
\begin{equation}
R = \sum\nolimits_{c \in C} \mathds{1}_{ \hat y_c > \hat y_L }
\end{equation}

The top-1 errors represents the error rate that the prediction is not the same with true label. This is useful to compare the performance of different models. However, it cannot show how how good the model is in general.
Sometimes the categories in a image may be ambiguous and it may be described as multiple labels. The order of several possible labels is ambiguous.
Using top-5 errors provides a more general understanding of learning ability of the model. 

\section{t-SNE}
\paragraph{(a)}

\paragraph{(b)}


\section{Proximal gradient decent}

\paragraph{(a)}
\paragraph{(b)}
\paragraph{(c)}
\paragraph{(d)}


\end{document}